{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title:\n",
    "### Fake News Classification Web app \n",
    "\n",
    " \n",
    "![fake news classifier](img/fake_news1.jpeg) \n",
    "![fake news classifier](img/fake2.jpeg) \n",
    "\n",
    "\n",
    "_by Team APACHE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Brief\n",
    "\n",
    "__Fake News A Real Problem__ - _The plague eating up social media_\n",
    "\n",
    "\n",
    "The destructive and catastrophic import of fake news can not be overemphasis and utterly underestimated, Though fake news start subtle and goes unnoticeable in the early stages, but when allow to breed, birth violent outcomes which is capable of instigating social, political wars, and capable of causing psychological effect on individuals targeted at, especially today, amid a pandemic, social media platforms are being used to dish out misinformation at lightning speed. One thing we can do is to avoid news altogether which seems nearly impossible or  one can utilize tools such as those of machine learning to fight the fatigue of fake new - __This is the intent of this project__\n",
    "\n",
    "## Project Scope And Boundary\n",
    "*  Kaggle fake news twitter dataset was used for this analysis. link [fake_news_dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)\n",
    "*  The news niche focus on polical news in the united states\n",
    "*  The news article examined in the dataset is 2 years old.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Aim\n",
    "\n",
    "The objective of this article is to outline end-to-end steps on building and training a machine learning model model to classify fake and true news using the best performing algorithm and deploying this model using Streamlit.</br>\n",
    "\n",
    "\n",
    "The dataset source is from Kaggle : Fake News dataset from the InClass Prediction Competition.</br>\n",
    "All notebooks and scripts used can be found on apache GitHub repo [apache-21](https://github.com/apache-21/fake_news_detection/tree/main/fake_new_detection_app). This article will illustrate 7 steps which are outline in the project workflow section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C3D0IlEKAQt"
   },
   "source": [
    "## Project Workflow :\n",
    "![fake news classifier](img/project_flow.png) \n",
    "1. Data Source : Data gotten from kaggle.com\n",
    "2. Data Preprocessing of text\n",
    "    * a. Exploratory Data Analysis\n",
    "    * b. Data cleaning and feature engineering\n",
    "    * c. Visualization\n",
    "3. Model Selection and Evaluation\n",
    "4. Data Pipeline\n",
    "5. Model Deployment\n",
    "6. Consolidation and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ehL3zMgMy7F"
   },
   "source": [
    "### Data Preprocessing\n",
    "#### Library use for project development:\n",
    "   * Pandas for data analysis\n",
    "            \n",
    "   * numpy for numerical computation\n",
    "   * matplotlib for visualisation\n",
    "   * spacy for information extraction to perform such as (NER, POS tagging, dependency parsing, word vectors \n",
    "   * nltk for text preprocessing, converting text into numbers for the model.\n",
    "   * Seaborn for visualization\n",
    "   * textblob for text preprocessing, such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation\n",
    "   * re library for to search and find patterns in the tweets\n",
    "   * wordcloud which is a  visualization technique for text data wherein each word is picturized with its importance in the context or its frequency.\n",
    "   * pickle to save the model and acess it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Methodology\n",
    "\n",
    "#### Preparing the dataset\n",
    "The dataset from Kaggle is provided in 2 CSV files which are already classified between true and fake news. The dataset was loaded using the pandas library however since it is textual data we carried out data cleaning, pre-processing, EDA and model-building operations.\n",
    "\n",
    "Below is an overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news = pd.read_csv('Fake.csv')\n",
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.644986Z",
     "iopub.status.busy": "2021-11-09T21:30:13.644746Z",
     "iopub.status.idle": "2021-11-09T21:30:13.676573Z",
     "shell.execute_reply": "2021-11-09T21:30:13.675880Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.644952Z"
    },
    "id": "gXnJ6FnPFUci",
    "outputId": "52041b19-0354-464a-d6f8-9be30c9a7917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n"
     ]
    }
   ],
   "source": [
    "fake_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The dataset was examine for missing values, and it is interesting that there are no missing values</br>\n",
    " we have a dataset of 4 features and 23481 observation\n",
    "\n",
    "More so, upon examining the dataset, it is observe that there are some words  present in the data which are irrelavant to the model, and as result we use reqular expression to get such unwanted patterns of text in the data and this is utilize in a function to search and find such words and phrase which is then filter it from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.679774Z",
     "iopub.status.busy": "2021-11-09T21:30:13.679494Z",
     "iopub.status.idle": "2021-11-09T21:30:13.685161Z",
     "shell.execute_reply": "2021-11-09T21:30:13.684326Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.679738Z"
    },
    "id": "XRDPe5d5MBaG"
   },
   "source": [
    "The re.sub() function is used to replace occurrences of a particular sub-string with another sub-string.<br>\n",
    "\n",
    "def operate_on_word(text): <br>\n",
    " &nbsp;&nbsp;   &emsp;text = re.sub('\\w*\\d\\w*', '',<br> \n",
    " &nbsp; &nbsp;&nbsp;&nbsp;              &emsp;re.sub('\\n', '',<br>\n",
    " &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;            &emsp;re.sub('[%s]' % re.escape(string.punctuation), '', <br>\n",
    " &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;          &emsp;re.sub('<.*?>+', '', <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;                   &emsp;re.sub('https?://\\S+|www\\.\\S+', '', <br>&nbsp;\n",
    " &nbsp;  &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;  &nbsp; &nbsp;&nbsp;         &emsp;re.sub(\"\\\\W\", ' ', <br>\n",
    " &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;                            &nbsp;&emsp;re.sub('\\[.*?\\]', '', text.lower())))))))<br>\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.687549Z",
     "iopub.status.busy": "2021-11-09T21:30:13.686977Z",
     "iopub.status.idle": "2021-11-09T21:30:31.169119Z",
     "shell.execute_reply": "2021-11-09T21:30:31.168462Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.687512Z"
    },
    "id": "0ekZYKiNJyh1",
    "outputId": "8ce21455-c2c2-4fc4-f276-c9c4ebbfb8ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59145324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake_corpus - patterns filter using the helper function \n",
    "fake_corpus = ' '.join(fake_news.text.apply(operate_on_word))\n",
    "\n",
    "len(fake_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the WordCloud from the preprocess dataset\n",
    " WordCloud : is a visualization technique for text data wherein each word is picturized with its importance in the context or its frequency. However to generate the wordcloud there is a need to define stopwords.\n",
    " \n",
    "\n",
    "In this entire process of generating a word cloud or processing any text data, we will always have a set of words that is not much of a concern to us. Words that belong to this category of “futile” words include is, was, for, of, it, a, the, etc. As a process of filtering data, we use stopwords to remove useless words.\n",
    "\n",
    "__Below is the WordCloud generated from the preprocess fake news dataset:__\n",
    " ![wordcloud](img/WordCloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Subsequenly__\n",
    "The spacy library was utilize for part of speech dependency tagging - This allows us know and understand the different part of speech in the text data and how there interdependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, the content of the true news data was examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:05.540342Z",
     "iopub.status.busy": "2021-11-09T21:31:05.540053Z",
     "iopub.status.idle": "2021-11-09T21:31:06.907542Z",
     "shell.execute_reply": "2021-11-09T21:31:06.906860Z",
     "shell.execute_reply.started": "2021-11-09T21:31:05.540303Z"
    },
    "id": "EcjmJqJmRH-C",
    "outputId": "9b4b8dd4-f5a7-440d-ceec-27916ae97027"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genuine_news = pd.read_csv('True.csv')\n",
    "\n",
    "genuine_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Class and the Subject of the news Content in the dataset\n",
    "The fake news data set and genuine news where merge together and the visualization of the class and subject to the category of news in the dataset with the respective frequency is shown below:\n",
    "\n",
    "![classifying news based subject](img/viz1.png) \n",
    "\n",
    "\n",
    "__Observation__\n",
    "* the domain subject appears to be heavy on polical news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Performing Name Entity Recognition on the data\n",
    "Named entity recognition (NER) ‒ also called entity identification or entity extraction ‒ is a natural language processing (NLP) technique that automatically identifies named entities in a text and classifies them into predefined categories. \n",
    "\n",
    "Name entity recognition was carried out on the text data to extract key names and entities present in the dataset and the below is the visualization\n",
    "\n",
    "\n",
    "![fake news classifier](img/name_entity.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fake news and genuine dataset were merge together using the pd.concat library and a new dataframe is form (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:06.990828Z",
     "iopub.status.busy": "2021-11-09T21:31:06.990509Z",
     "iopub.status.idle": "2021-11-09T21:31:07.004990Z",
     "shell.execute_reply": "2021-11-09T21:31:07.003777Z",
     "shell.execute_reply.started": "2021-11-09T21:31:06.990792Z"
    },
    "id": "o4w69p-iSCQp",
    "outputId": "a3a79045-d36e-4c40-fa4e-567cabb39529"
   },
   "source": [
    "### Feature Engineering\n",
    "Inorder to draw more insight from the dataset new features were engineered such as\n",
    "\n",
    "* Polarity : which is an output of the textblob which gives the ability of knowing the sentinent in each tweet.\n",
    "\n",
    "* text_len : gives the length of each text or tweet size\n",
    "\n",
    "* text_word_count : gives  the count of word in the text \n",
    "\n",
    "* title_len : gives the size of the tweet title\n",
    "\n",
    "### Transforming text data into numerical values \n",
    "\n",
    "Machine learning algorithm thrive on numerical values hence library such  the countvectorizer,bag of words model was use  to achieve the numerical transformation.\n",
    "\n",
    "A helper function get_top_n_words is define for  this numerical transformation and to also get the top words with visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:33:17.036108Z",
     "iopub.status.busy": "2021-11-09T21:33:17.035626Z",
     "iopub.status.idle": "2021-11-09T21:33:17.050356Z",
     "shell.execute_reply": "2021-11-09T21:33:17.049671Z",
     "shell.execute_reply.started": "2021-11-09T21:33:17.036067Z"
    },
    "id": "wfrMONSUpi3Y"
   },
   "source": [
    "def get_top_n_words(corpus, name='text', n=None):<br>\n",
    "     &nbsp;&nbsp;&nbsp;&emsp; corpus_fake = corpus[df['genuine'] == 1].astype(str)<br>\n",
    "     &nbsp;&nbsp;&nbsp;&emsp; corpus_true = corpus[df['genuine'] == 0].astype(str)\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus_fake)\n",
    "    bag_of_words = vec.transform(corpus_fake)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq_fake = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_fake =sorted(words_freq_fake, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus_true)\n",
    "    bag_of_words = vec.transform(corpus_true)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq_true = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_true =sorted(words_freq_true, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    df_fake = pd.DataFrame(words_freq_fake[:n],columns = ['text', 'count'])\n",
    "    df_true = pd.DataFrame(words_freq_true[:n],columns = ['text', 'count'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,8))\n",
    "    ax1.bar(df_fake['text'], df_fake['count'])\n",
    "    ax1.set_xticklabels(df_fake['text'])\n",
    "    ax1.set(xlabel='top 10 most frequent terms for fake news', ylabel='count')\n",
    "    ax2.bar(df_true['text'], df_true['count'])\n",
    "    ax2.set_xticklabels(df_true['text'])\n",
    "    ax2.set(xlabel='top 10 most frequent terms for true news', ylabel='count')\n",
    "    plt.suptitle('Comparision between the top 10 most frequent terms (fake/true)')\n",
    "\n",
    "    fig.savefig(f'most_freq_{name}.png')\n",
    "    \n",
    "    \n",
    "visualization for the most frequent text \n",
    "![fake news classifier](img/most_freq_text.png) \n",
    "\n",
    "\n",
    "visualization for the most frequent title in the data\n",
    "![fake news classifier](img/most_freq_title.png) \n",
    "\n",
    "#### Observation from the above visualization:\n",
    " __Based on the comparison between the top 10 frequent words in titles and news text, we can infer that both fake and true news is dominated by news relating to politics and more specifically, the subject being heavily related to American politics is shared between true and fake news. This would result in the model been biased to classifying news that relates to only American Politics and probably of that time frame. To mitigate this bias more recent data and diverse news data would be needed__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Data Pipeline\n",
    "\n",
    "The dataset was splitted into test and train set and an helper function is created to remove unwanted patterns  in the dataset then passed to through a model through another helper function that both transform and preprocess  the text into numerical values then make prediction \n",
    "\n",
    "### List of Classifier Models Use\n",
    "Classical machine learning algorithms were utilize for this classifier, thne a deep learning model lstm was also used.\n",
    "\n",
    "* Naive bayes - multinomial\n",
    "* Logistics regression\n",
    "* Random forest classifier\n",
    "* Gradient boosting classifier\n",
    "* Lstm - Deep learning model\n",
    "\n",
    "Below is the helper function  - a data pipeline that take the dataset or tweet then preprocess it and feed it into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.818753Z",
     "iopub.status.busy": "2021-11-09T21:34:25.816144Z",
     "iopub.status.idle": "2021-11-09T21:34:25.826612Z",
     "shell.execute_reply": "2021-11-09T21:34:25.825752Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.818717Z"
    }
   },
   "source": [
    "# data pipeline  which entails transformimng the data to numerical data with removal of irrelevant pattern present.\n",
    "def load_classifier(clf, X_train, X_valid, y_train, y_valid):</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;pipe_clf = make_pipeline(FunctionTransformer(transform_word), </br>\n",
    "&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TfidfVectorizer(ngram_range=(1, 2), max_features=5000),clf)</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;pipe_clf.fit(X_train, y_train)\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;y_pred = pipe_clf.predict(X_valid)\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;probas = pipe_clf.predict_proba(X_valid)\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;return pipe_clf, y_pred, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.833588Z",
     "iopub.status.busy": "2021-11-09T21:34:25.831099Z",
     "iopub.status.idle": "2021-11-09T21:48:53.526822Z",
     "shell.execute_reply": "2021-11-09T21:48:53.525982Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.833553Z"
    }
   },
   "source": [
    "classifiers = [MultinomialNB(), LogisticRegression(),RandomForestClassifier(), GradientBoostingClassifier()] </br>\n",
    "\n",
    "model_list, preds_list, probas_list = [], [], [] </br>\n",
    "\n",
    "for clf in classifiers:</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;   model, pred, probas = load_classifier(clf, X_train_all, X_test, y_train_all, y_test)</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;    model_list.append(model)</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;     preds_list.append(pred)</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;    probas_list.append(probas)</br>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The performance of the model on the validation is examine using the evaluation metrics below:\n",
    "\n",
    "* confusion matrix\n",
    "* Accuracy score\n",
    "* Precision, \n",
    "* Recall, \n",
    "* f1score\n",
    "\n",
    "### Training a Deep neural network( LSTM) on the dataset\n",
    "\n",
    "The dataset was first preprocess for the neural netowrk then train using the lstm models\n",
    "\n",
    "The models were evalauted and the result shown in the table below:\n",
    "\n",
    "![model evaluation](img/all_metrics_df(1).png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The random forest seems to outperform other classifer thus, it will be consider as the choice algorithm use in the deloyement phase__\n",
    "\n",
    "__However, it will be interesting to check the perfomance of the deep learning model (lstm) on the dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MULTINOMIALNB</td>\n",
       "      <td>0.948058</td>\n",
       "      <td>0.937729</td>\n",
       "      <td>0.956116</td>\n",
       "      <td>0.946833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOGISTICREGRESSION</td>\n",
       "      <td>0.990515</td>\n",
       "      <td>0.989739</td>\n",
       "      <td>0.990663</td>\n",
       "      <td>0.990201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RANDOMFORESTCLASSIFIER</td>\n",
       "      <td>0.998193</td>\n",
       "      <td>0.999065</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>0.998131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRADIENTBOOSTINGCLASSIFIER</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.996272</td>\n",
       "      <td>0.998133</td>\n",
       "      <td>0.997201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM_model</td>\n",
       "      <td>0.993677</td>\n",
       "      <td>0.994387</td>\n",
       "      <td>0.992530</td>\n",
       "      <td>0.993458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unnamed: 0  Accuracy  Precision    Recall  F1_Score\n",
       "0               MULTINOMIALNB  0.948058   0.937729  0.956116  0.946833\n",
       "1          LOGISTICREGRESSION  0.990515   0.989739  0.990663  0.990201\n",
       "2      RANDOMFORESTCLASSIFIER  0.998193   0.999065  0.997199  0.998131\n",
       "3  GRADIENTBOOSTINGCLASSIFIER  0.997290   0.996272  0.998133  0.997201\n",
       "4                  LSTM_model  0.993677   0.994387  0.992530  0.993458"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval_metrics = pd.read_csv('img/all_metrics_df.csv')\n",
    " \n",
    "Eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the boss model and Saving the model using pickle\n",
    "\n",
    "The random forest seems to be the boss, hence it is chosen and the model is save using pickle and can then be use for future prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:48:54.419481Z",
     "iopub.status.busy": "2021-11-09T21:48:54.419288Z",
     "iopub.status.idle": "2021-11-09T21:49:00.472273Z",
     "shell.execute_reply": "2021-11-09T21:49:00.471495Z",
     "shell.execute_reply.started": "2021-11-09T21:48:54.419457Z"
    }
   },
   "source": [
    "model_file_list = [] </br>\n",
    "for model in model_list:</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;    model_name = model.steps[-1][0]</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;    filename = f'{model_name}_model.pkl'</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;    model_file_list.append(filename)</br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;    pickle.dump(model, open(filename, 'wb'))</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployement \n",
    "\n",
    "The fake news classification app is then deploy on the web using streamlit and readily available to end users for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:00.521818Z",
     "iopub.status.busy": "2021-11-09T21:49:00.521568Z",
     "iopub.status.idle": "2021-11-09T21:49:20.980692Z",
     "shell.execute_reply": "2021-11-09T21:49:20.979835Z",
     "shell.execute_reply.started": "2021-11-09T21:49:00.521784Z"
    },
    "id": "raU97o126kEt"
   },
   "source": [
    "\n",
    "### Fake news Classifiaction source code link : [apache-21](https://github.com/apache-21/fake_news_detection/blob/main/fake_new_detection_app/fake-news-5.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
