{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title:\n",
    "### Fake News Classification Web app \n",
    "\n",
    " \n",
    "![fake news classifier](img/fake_news1.jpeg) \n",
    "![fake news classifier](img/fake2.jpeg) \n",
    "\n",
    "\n",
    "_by Team APACHE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Brief\n",
    "\n",
    "__Fake News A Real Problem__ - _The plague eating up social media_\n",
    "\n",
    "\n",
    "The destructive and catastrophic import of fake news can not be overemphasis and utterly underestimate, Though fake news start subtle and goes unnoticeable in the early stage but when allow to breed birth violent outcomes and it is capable of instigating social, political wars, and capable of causing psychological effect on individuals targeted at, Especially today, amid a pandemic, social media platforms are being used to dish out misinformation at lightning speed. One thing we can do is to avoid news altogether or use tools such as those of machine learning to fight the fatigue of fake new and this is the what this project tend to achieve.\n",
    "\n",
    "## Project Scope And Boundary\n",
    "*  Kaggle fake news twitter dataset was used for this analysis. link https://www.kaggle.com/c/fake-news/data\n",
    "*  The news niche focus on polical news in the united states\n",
    "*  The news article examined in the dataset is 4years old.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Aim\n",
    "\n",
    "The objective of this article is to outline end-to-end steps on building and training a machine learning model model to classify fake and true news using the best performing algorithm and deploying this model using Streamlit. The dataset that will be used will be Kaggle’s Fake News dataset from the InClass Prediction Competition. All notebooks and scripts used can be found on apache GitHub repo. This article will illustrate 7 steps which are outline in the project workflow and methodology section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C3D0IlEKAQt"
   },
   "source": [
    "## Project Workflow :\n",
    "![fake news classifier](img/project_flow.png) \n",
    "1. Data Source : Data gotten from kaggle.com\n",
    "2. Data Preprocessing of text\n",
    "    a. Exploratory Data Analysis\n",
    "    b. Data cleaning and feature engineering\n",
    "    b. Visualization\n",
    "3. Model Selection and Evaluation\n",
    "4. Data Pipeline\n",
    "5. Model Deployment\n",
    "6. Consolidation and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ehL3zMgMy7F"
   },
   "source": [
    "### Data Preprocessing\n",
    "#### Library use for project development:\n",
    "        *Pandas for data analysis\n",
    "            \n",
    "        * numpy for numerical computation\n",
    "        * matplotlib for visualisation\n",
    "        * spacy for information extraction to perform such as (NER, POS tagging, dependency parsing, word vectors and more.\n",
    "        * nltk for text preprocessing, converting text into numbers for the model.\n",
    "        * Seaborn for visualization\n",
    "        * textblob for text preprocessing, such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation\n",
    "        * re library for to search and find patterns in the tweets\n",
    "        * wordcloud which is a  visualization technique for text data wherein each word is picturized with its importance in the context or its frequency.\n",
    "        * pickle to save the model and acess it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Methodology\n",
    "\n",
    "Preparing the dataset\n",
    "The dataset from Kaggle is provided in 2 CSV files which are already classified between true and fake news. The dataset was loaded using the pandas library however since it is textual data we carried out data cleaning, pre-processing, EDA and model-building oper\n",
    "\n",
    "The is the loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:12.092514Z",
     "iopub.status.busy": "2021-11-09T21:30:12.090701Z",
     "iopub.status.idle": "2021-11-09T21:30:13.643403Z",
     "shell.execute_reply": "2021-11-09T21:30:13.642681Z",
     "shell.execute_reply.started": "2021-11-09T21:30:12.092481Z"
    },
    "id": "-xffhFF3JvIV",
    "outputId": "b477fffe-bc1d-47cf-a44d-7a385400e78a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.644986Z",
     "iopub.status.busy": "2021-11-09T21:30:13.644746Z",
     "iopub.status.idle": "2021-11-09T21:30:13.676573Z",
     "shell.execute_reply": "2021-11-09T21:30:13.675880Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.644952Z"
    },
    "id": "gXnJ6FnPFUci",
    "outputId": "52041b19-0354-464a-d6f8-9be30c9a7917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n"
     ]
    }
   ],
   "source": [
    "fake_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying information about the dataset shows that there are no missing values we have a dataset of 4 features and 23481 observation\n",
    "\n",
    "More so, examining the dataset some words are present in the tweet which are irrelavant to the model. An a result we use reqular expression by writing an helper function in python to search and find such words and phrase then filter it out from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.679774Z",
     "iopub.status.busy": "2021-11-09T21:30:13.679494Z",
     "iopub.status.idle": "2021-11-09T21:30:13.685161Z",
     "shell.execute_reply": "2021-11-09T21:30:13.684326Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.679738Z"
    },
    "id": "XRDPe5d5MBaG"
   },
   "outputs": [],
   "source": [
    "#The re.sub() function is used to replace occurrences of a particular sub-string \n",
    "# with another sub-string.\n",
    "def operate_on_word(text):\n",
    "    text = re.sub('\\w*\\d\\w*', '', \n",
    "                re.sub('\\n', '',\n",
    "              re.sub('[%s]' % re.escape(string.punctuation), '', \n",
    "                re.sub('<.*?>+', '', \n",
    "                       re.sub('https?://\\S+|www\\.\\S+', '', \n",
    "                              re.sub(\"\\\\W\", ' ', \n",
    "                                     re.sub('\\[.*?\\]', '', text.lower())))))))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:13.687549Z",
     "iopub.status.busy": "2021-11-09T21:30:13.686977Z",
     "iopub.status.idle": "2021-11-09T21:30:31.169119Z",
     "shell.execute_reply": "2021-11-09T21:30:31.168462Z",
     "shell.execute_reply.started": "2021-11-09T21:30:13.687512Z"
    },
    "id": "0ekZYKiNJyh1",
    "outputId": "8ce21455-c2c2-4fc4-f276-c9c4ebbfb8ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59145324"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake_corpus - patterns filter using the helper function \n",
    "fake_corpus = ' '.join(fake_news.text.apply(operate_on_word))\n",
    "\n",
    "len(fake_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let view a content in fake_corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:31.170751Z",
     "iopub.status.busy": "2021-11-09T21:30:31.170504Z",
     "iopub.status.idle": "2021-11-09T21:30:31.174809Z",
     "shell.execute_reply": "2021-11-09T21:30:31.173863Z",
     "shell.execute_reply.started": "2021-11-09T21:30:31.170716Z"
    },
    "id": "X8bpq3JbOile"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "first_fake_news = fake_news['text'][0]\n",
    "first_fake_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:30:31.176797Z",
     "iopub.status.busy": "2021-11-09T21:30:31.176221Z",
     "iopub.status.idle": "2021-11-09T21:31:04.849347Z",
     "shell.execute_reply": "2021-11-09T21:31:04.848730Z",
     "shell.execute_reply.started": "2021-11-09T21:30:31.176759Z"
    },
    "id": "zjDl6cEgQM2V",
    "outputId": "f95a09d7-30ed-422d-c9e9-8bc96cb761fe"
   },
   "outputs": [],
   "source": [
    "# Generate WORDCLOUD\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "cloud_news = WordCloud(width=800,\n",
    "                       height=800,\n",
    "                       stopwords=stopwords,\n",
    "                       background_color='white',\n",
    "                       min_font_size=10).generate(fake_corpus)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.imshow(cloud_news, interpolation='bilinear')\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "# fig.savefig('WordCloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:04.850564Z",
     "iopub.status.busy": "2021-11-09T21:31:04.850351Z",
     "iopub.status.idle": "2021-11-09T21:31:05.538068Z",
     "shell.execute_reply": "2021-11-09T21:31:05.537190Z",
     "shell.execute_reply.started": "2021-11-09T21:31:04.850536Z"
    },
    "id": "bHoKmNpOWYZX",
    "outputId": "4242a188-b76b-48a0-fb95-1f1659052fdf"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "title_doc1 = nlp('NLP for Fake News Detection')\n",
    "# sentences_spans = list(doc.sents)\n",
    "\n",
    "displacy.render(title_doc1, style='dep', jupyter=True)\n",
    "\n",
    "# output_path = Path(\"dependency_plot.svg\") # you can keep there only \"dependency_plot.svg\" if you want to save it in the same folder where you run the script \n",
    "# output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:05.540342Z",
     "iopub.status.busy": "2021-11-09T21:31:05.540053Z",
     "iopub.status.idle": "2021-11-09T21:31:06.907542Z",
     "shell.execute_reply": "2021-11-09T21:31:06.906860Z",
     "shell.execute_reply.started": "2021-11-09T21:31:05.540303Z"
    },
    "id": "EcjmJqJmRH-C",
    "outputId": "9b4b8dd4-f5a7-440d-ceec-27916ae97027"
   },
   "outputs": [],
   "source": [
    "genuine_news = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\n",
    "\n",
    "genuine_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:06.910890Z",
     "iopub.status.busy": "2021-11-09T21:31:06.910694Z",
     "iopub.status.idle": "2021-11-09T21:31:06.942026Z",
     "shell.execute_reply": "2021-11-09T21:31:06.941282Z",
     "shell.execute_reply.started": "2021-11-09T21:31:06.910866Z"
    },
    "id": "5ltXR8VGpiKl",
    "outputId": "f49881a6-ec10-4491-e305-12d4eccddd89"
   },
   "outputs": [],
   "source": [
    "# Names Entity Visualization\n",
    "\n",
    "# genuine_news.text[(478 < genuine_news.text.apply(len)) & (genuine_news.text.apply(len) < 480)].index\n",
    "\n",
    "indices = [484, 968,  1257,  3799, 7603, 7628,  9147, 12196, 12735,\n",
    "            16536, 17528, 19804, 20393]\n",
    "displacy.render(nlp(genuine_news.text[np.random.choice(indices, 1)[0]]), style='ent', jupyter=True)\n",
    "\n",
    "# output_path = Path(\"named_entity_plot.svg\")  \n",
    "# output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:06.943621Z",
     "iopub.status.busy": "2021-11-09T21:31:06.943312Z",
     "iopub.status.idle": "2021-11-09T21:31:06.967845Z",
     "shell.execute_reply": "2021-11-09T21:31:06.967111Z",
     "shell.execute_reply.started": "2021-11-09T21:31:06.943585Z"
    },
    "id": "eArM2LFMILzL"
   },
   "outputs": [],
   "source": [
    "fake_news['genuine'] = 0\n",
    "\n",
    "fake_news = fake_news.drop(fake_news[fake_news['text'] == ' '].index)\n",
    "\n",
    "print(len(fake_news))\n",
    "\n",
    "fake_deploy = fake_news.sample(n=5)\n",
    "\n",
    "print(fake_deploy.head())\n",
    "\n",
    "fake_news = fake_news.drop(fake_deploy.index)\n",
    "\n",
    "# fake_news['text'] =fake_news.text.apply(operate_on_word)\n",
    "# fake_news['title'] =fake_news.title.apply(operate_on_word)\n",
    "\n",
    "print(len(fake_news))\n",
    "\n",
    "part_1 = fake_news[['title', 'text', 'genuine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:06.969342Z",
     "iopub.status.busy": "2021-11-09T21:31:06.969063Z",
     "iopub.status.idle": "2021-11-09T21:31:06.989420Z",
     "shell.execute_reply": "2021-11-09T21:31:06.988642Z",
     "shell.execute_reply.started": "2021-11-09T21:31:06.969308Z"
    },
    "id": "GOa4Z896RaQS"
   },
   "outputs": [],
   "source": [
    "genuine_news['genuine'] = 1\n",
    "\n",
    "genuine_news = genuine_news.drop(genuine_news[genuine_news['text'] == ' '].index)\n",
    "\n",
    "print(len(genuine_news))\n",
    "\n",
    "genuine_deploy = genuine_news.sample(n=5)\n",
    "\n",
    "genuine_news = genuine_news.drop(genuine_deploy.index)\n",
    "\n",
    "# genuine_news['text'] = genuine_news.text.apply(operate_on_word)\n",
    "# genuine_news['title'] = genuine_news.title.apply(operate_on_word)\n",
    "\n",
    "print(len(genuine_news))\n",
    "\n",
    "part_2 = genuine_news[['title', 'text', 'genuine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:06.990828Z",
     "iopub.status.busy": "2021-11-09T21:31:06.990509Z",
     "iopub.status.idle": "2021-11-09T21:31:07.004990Z",
     "shell.execute_reply": "2021-11-09T21:31:07.003777Z",
     "shell.execute_reply.started": "2021-11-09T21:31:06.990792Z"
    },
    "id": "o4w69p-iSCQp",
    "outputId": "a3a79045-d36e-4c40-fa4e-567cabb39529"
   },
   "outputs": [],
   "source": [
    "all_parts = pd.concat([part_1, part_2], ignore_index=True, axis = 0)\n",
    "\n",
    "all_parts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:07.006485Z",
     "iopub.status.busy": "2021-11-09T21:31:07.006225Z",
     "iopub.status.idle": "2021-11-09T21:31:07.018563Z",
     "shell.execute_reply": "2021-11-09T21:31:07.017619Z",
     "shell.execute_reply.started": "2021-11-09T21:31:07.006453Z"
    },
    "id": "pl6rRkRPScpn",
    "outputId": "e7f803f2-22ca-44de-92b5-8f0325a72893"
   },
   "outputs": [],
   "source": [
    "counts_df = all_parts['genuine'].value_counts().to_frame()\n",
    "\n",
    "counts_df.index = ['Fake', 'Genuine']\n",
    "counts_df.columns = ['Count']\n",
    "\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:07.020089Z",
     "iopub.status.busy": "2021-11-09T21:31:07.019881Z",
     "iopub.status.idle": "2021-11-09T21:31:07.041936Z",
     "shell.execute_reply": "2021-11-09T21:31:07.041161Z",
     "shell.execute_reply.started": "2021-11-09T21:31:07.020066Z"
    },
    "id": "0_nEiBlLSoSr",
    "outputId": "63db6ecc-dcc8-410b-ea39-30843f9a3d27"
   },
   "outputs": [],
   "source": [
    "# Shuffle Dataset\n",
    "df = all_parts.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:07.044370Z",
     "iopub.status.busy": "2021-11-09T21:31:07.043045Z",
     "iopub.status.idle": "2021-11-09T21:31:07.051349Z",
     "shell.execute_reply": "2021-11-09T21:31:07.050544Z",
     "shell.execute_reply.started": "2021-11-09T21:31:07.044332Z"
    },
    "id": "KJMgXmdDThQF",
    "outputId": "dcc23f9d-71e2-4930-9f6b-3b1297766a7a"
   },
   "outputs": [],
   "source": [
    "df.genuine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:31:07.053118Z",
     "iopub.status.busy": "2021-11-09T21:31:07.052687Z",
     "iopub.status.idle": "2021-11-09T21:33:17.028389Z",
     "shell.execute_reply": "2021-11-09T21:33:17.027621Z",
     "shell.execute_reply.started": "2021-11-09T21:31:07.053082Z"
    },
    "id": "CE1vBhyLpgNB"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "df['polarity'] = df['text'].map(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "df['text_len'] = df['text'].astype(str).map(len)\n",
    "df['text_word_count'] = df['text'].map(lambda x: len(str(x).split()))\n",
    "df['title_len'] = df['title'].astype(str).map(len)\n",
    "df['title_word_count'] = df['title'].map(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:33:17.029834Z",
     "iopub.status.busy": "2021-11-09T21:33:17.029576Z",
     "iopub.status.idle": "2021-11-09T21:33:17.034493Z",
     "shell.execute_reply": "2021-11-09T21:33:17.033797Z",
     "shell.execute_reply.started": "2021-11-09T21:33:17.029783Z"
    },
    "id": "vCqN2skX0c1Q"
   },
   "outputs": [],
   "source": [
    "# df2.to_csv('/content/drive/MyDrive/News_NLP/news_cleaned_with_feats.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:33:17.036108Z",
     "iopub.status.busy": "2021-11-09T21:33:17.035626Z",
     "iopub.status.idle": "2021-11-09T21:33:17.050356Z",
     "shell.execute_reply": "2021-11-09T21:33:17.049671Z",
     "shell.execute_reply.started": "2021-11-09T21:33:17.036067Z"
    },
    "id": "wfrMONSUpi3Y"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, name='text', n=None):\n",
    "    corpus_fake = corpus[df['genuine'] == 1].astype(str)\n",
    "    corpus_true = corpus[df['genuine'] == 0].astype(str)\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus_fake)\n",
    "    bag_of_words = vec.transform(corpus_fake)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq_fake = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_fake =sorted(words_freq_fake, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus_true)\n",
    "    bag_of_words = vec.transform(corpus_true)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq_true = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_true =sorted(words_freq_true, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    df_fake = pd.DataFrame(words_freq_fake[:n],columns = ['text', 'count'])\n",
    "    df_true = pd.DataFrame(words_freq_true[:n],columns = ['text', 'count'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,8))\n",
    "    ax1.bar(df_fake['text'], df_fake['count'])\n",
    "    ax1.set_xticklabels(df_fake['text'])\n",
    "    ax1.set(xlabel='top 10 most frequent terms for fake news', ylabel='count')\n",
    "    ax2.bar(df_true['text'], df_true['count'])\n",
    "    ax2.set_xticklabels(df_true['text'])\n",
    "    ax2.set(xlabel='top 10 most frequent terms for true news', ylabel='count')\n",
    "    plt.suptitle('Comparision between the top 10 most frequent terms (fake/true)')\n",
    "\n",
    "    fig.savefig(f'most_freq_{name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:33:17.052387Z",
     "iopub.status.busy": "2021-11-09T21:33:17.051495Z",
     "iopub.status.idle": "2021-11-09T21:34:23.656769Z",
     "shell.execute_reply": "2021-11-09T21:34:23.656099Z",
     "shell.execute_reply.started": "2021-11-09T21:33:17.052353Z"
    },
    "id": "VI73nzv2xllR",
    "outputId": "ed73365a-e9b8-4ad3-e504-6825f9b5509b"
   },
   "outputs": [],
   "source": [
    "df2 = df.copy(deep=True)\n",
    "\n",
    "df2['text'] = df2.text.apply(operate_on_word)\n",
    "df2['title'] = df2.title.apply(operate_on_word)\n",
    "\n",
    "get_top_n_words(corpus=df2['text'], name='text', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:23.658695Z",
     "iopub.status.busy": "2021-11-09T21:34:23.657896Z",
     "iopub.status.idle": "2021-11-09T21:34:25.720014Z",
     "shell.execute_reply": "2021-11-09T21:34:25.719209Z",
     "shell.execute_reply.started": "2021-11-09T21:34:23.658656Z"
    },
    "id": "MqsMPnM_xloF",
    "outputId": "7b92f57f-b02d-4ec0-b64f-9d375833e3b8"
   },
   "outputs": [],
   "source": [
    "get_top_n_words(corpus=df2['title'], name='title', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpjRO7IW10TG"
   },
   "source": [
    "#### Based on the comparison between the top 10 frequent words in titles and news text, we can infer that both fake and true news is dominated by news relating to politics and more specifically, the subject being heavily related to American politics is shared between true and fake news. This would result in the model been biased to classifying news that relates to only American Politics and probably of that time frame. To mitigate this bias more recent data and diverse news data would be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.726499Z",
     "iopub.status.busy": "2021-11-09T21:34:25.724350Z",
     "iopub.status.idle": "2021-11-09T21:34:25.797575Z",
     "shell.execute_reply": "2021-11-09T21:34:25.796935Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.726462Z"
    },
    "id": "YuTSiuomUVku",
    "outputId": "adce0750-e4d8-48ad-d586-fe17ee3d38a6"
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['genuine']\n",
    "\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(X, y, test_size=0.05, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=0.2)\n",
    "\n",
    "print(X_train.shape[0], X_valid.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.803135Z",
     "iopub.status.busy": "2021-11-09T21:34:25.801266Z",
     "iopub.status.idle": "2021-11-09T21:34:25.811635Z",
     "shell.execute_reply": "2021-11-09T21:34:25.811016Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.803098Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_word(texts):\n",
    "    texts_new = []\n",
    "    for text in texts:\n",
    "        text = re.sub('\\w*\\d\\w*', '', \n",
    "                  re.sub('\\n', '',\n",
    "                re.sub('[%s]' % re.escape(string.punctuation), '', \n",
    "                  re.sub('<.*?>+', '', \n",
    "                        re.sub('https?://\\S+|www\\.\\S+', '', \n",
    "                                re.sub(\"\\\\W\", ' ', \n",
    "                                      re.sub('\\[.*?\\]', '', text.lower())))))))\n",
    "        texts_new.append(text)\n",
    "    return np.array(texts_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.818753Z",
     "iopub.status.busy": "2021-11-09T21:34:25.816144Z",
     "iopub.status.idle": "2021-11-09T21:34:25.826612Z",
     "shell.execute_reply": "2021-11-09T21:34:25.825752Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.818717Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_classifier(clf, X_train, X_valid, y_train, y_valid):\n",
    "    pipe_clf = make_pipeline(FunctionTransformer(transform_word), \n",
    "                             TfidfVectorizer(ngram_range=(1, 2), max_features=5000),\n",
    "                             clf)\n",
    "    pipe_clf.fit(X_train, y_train)\n",
    "    y_pred = pipe_clf.predict(X_valid)\n",
    "    probas = pipe_clf.predict_proba(X_valid)\n",
    "    \n",
    "    return pipe_clf, y_pred, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:34:25.833588Z",
     "iopub.status.busy": "2021-11-09T21:34:25.831099Z",
     "iopub.status.idle": "2021-11-09T21:48:53.526822Z",
     "shell.execute_reply": "2021-11-09T21:48:53.525982Z",
     "shell.execute_reply.started": "2021-11-09T21:34:25.833553Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [MultinomialNB(), LogisticRegression(),\n",
    "               RandomForestClassifier(), GradientBoostingClassifier()]\n",
    "\n",
    "model_list, preds_list, probas_list = [], [], []\n",
    "\n",
    "for clf in classifiers:\n",
    "    model, pred, probas = load_classifier(clf, X_train_all, X_test, y_train_all, y_test)\n",
    "    model_list.append(model)\n",
    "    preds_list.append(pred)\n",
    "    probas_list.append(probas)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:48:53.528756Z",
     "iopub.status.busy": "2021-11-09T21:48:53.528513Z",
     "iopub.status.idle": "2021-11-09T21:48:53.538040Z",
     "shell.execute_reply": "2021-11-09T21:48:53.537190Z",
     "shell.execute_reply.started": "2021-11-09T21:48:53.528721Z"
    },
    "id": "xKLV0Wkn5TmX"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(model_list, preds_list, y_valid):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n",
    "    \n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    for ax, y_pred, model in zip(axes, preds_list, model_list):\n",
    "        cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "        cm_matrix = pd.DataFrame(data=cm, columns=['Predict Fake', 'Predict True'], \n",
    "                                     index=['Actual Fake', 'Actual True'])\n",
    "\n",
    "        sns.heatmap(cm_matrix, annot=True, fmt='d', ax =ax, cmap='YlGnBu')\n",
    "        ax.set_title(model.steps[-1][0])\n",
    "        \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:48:53.540000Z",
     "iopub.status.busy": "2021-11-09T21:48:53.539446Z",
     "iopub.status.idle": "2021-11-09T21:48:54.414856Z",
     "shell.execute_reply": "2021-11-09T21:48:54.414091Z",
     "shell.execute_reply.started": "2021-11-09T21:48:53.539959Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrices(model_list, preds_list, y_test)\n",
    "# model_list = [naive_model, logreg, rf_clf, svm_clf]\n",
    "\n",
    "# plot_confusion_matrices(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:48:54.419481Z",
     "iopub.status.busy": "2021-11-09T21:48:54.419288Z",
     "iopub.status.idle": "2021-11-09T21:49:00.472273Z",
     "shell.execute_reply": "2021-11-09T21:49:00.471495Z",
     "shell.execute_reply.started": "2021-11-09T21:48:54.419457Z"
    }
   },
   "outputs": [],
   "source": [
    "model_file_list = []\n",
    "for model in model_list:\n",
    "    model_name = model.steps[-1][0]\n",
    "    filename = f'{model_name}_model.pkl'\n",
    "    model_file_list.append(filename)\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:00.473804Z",
     "iopub.status.busy": "2021-11-09T21:49:00.473556Z",
     "iopub.status.idle": "2021-11-09T21:49:00.520106Z",
     "shell.execute_reply": "2021-11-09T21:49:00.519302Z",
     "shell.execute_reply.started": "2021-11-09T21:49:00.473770Z"
    }
   },
   "outputs": [],
   "source": [
    "X2 = df2['text']\n",
    "y2 = df2['genuine']\n",
    "\n",
    "X_train2_all, X_test2, y_train2_all, y_test2 = train_test_split(X2, y2,\n",
    "                                                            test_size=0.05,\n",
    "                                                            stratify=y, random_state=0)\n",
    "\n",
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_train2_all,\n",
    "                                                      y_train2_all,\n",
    "                                                      test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train2.shape[0], X_valid2.shape[0], X_test2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:00.521818Z",
     "iopub.status.busy": "2021-11-09T21:49:00.521568Z",
     "iopub.status.idle": "2021-11-09T21:49:20.980692Z",
     "shell.execute_reply": "2021-11-09T21:49:20.979835Z",
     "shell.execute_reply.started": "2021-11-09T21:49:00.521784Z"
    },
    "id": "raU97o126kEt"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train2)\n",
    "\n",
    "max_seq=400\n",
    "\n",
    "X_train_tf = tokenizer.texts_to_sequences(X_train2)\n",
    "X_train_tf = pad_sequences(X_train_tf, maxlen=max_seq)\n",
    "\n",
    "X_valid_tf = tokenizer.texts_to_sequences(X_valid2)\n",
    "X_valid_tf = pad_sequences(X_valid_tf, maxlen=max_seq)\n",
    "\n",
    "# encoder = TextVectorization(max_tokens=10000)\n",
    "# encoder.adapt(X_train)\n",
    "\n",
    "# vocab = encoder.get_vocabulary()\n",
    "\n",
    "# vocab[:10]\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:20.982555Z",
     "iopub.status.busy": "2021-11-09T21:49:20.982269Z",
     "iopub.status.idle": "2021-11-09T21:49:29.891411Z",
     "shell.execute_reply": "2021-11-09T21:49:29.890667Z",
     "shell.execute_reply.started": "2021-11-09T21:49:20.982519Z"
    },
    "id": "MpPkK1lc-Ut7"
   },
   "outputs": [],
   "source": [
    "embedding_dim=64\n",
    "\n",
    "model = Sequential([\n",
    "  Input(shape=(max_seq,), dtype='int32'),\n",
    "  Embedding(input_dim = len(word_index) + 1, output_dim = embedding_dim, name=\"embedding\"),\n",
    "  Conv1D(32, kernel_size=3, padding='same'),\n",
    "  MaxPooling1D(pool_size=2),\n",
    "  Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "  Bidirectional(LSTM(32, dropout=0.2)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:29.892909Z",
     "iopub.status.busy": "2021-11-09T21:49:29.892656Z",
     "iopub.status.idle": "2021-11-09T21:49:29.905662Z",
     "shell.execute_reply": "2021-11-09T21:49:29.904884Z",
     "shell.execute_reply.started": "2021-11-09T21:49:29.892877Z"
    },
    "id": "D8rPAI53_dDc",
    "outputId": "7d02c439-f020-4f69-87e5-3c661627bc21"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:29.907031Z",
     "iopub.status.busy": "2021-11-09T21:49:29.906672Z",
     "iopub.status.idle": "2021-11-09T21:49:31.162986Z",
     "shell.execute_reply": "2021-11-09T21:49:31.161308Z",
     "shell.execute_reply.started": "2021-11-09T21:49:29.906995Z"
    },
    "id": "3RWTQ5tX0VfS"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n",
    "\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:31.164367Z",
     "iopub.status.busy": "2021-11-09T21:49:31.164098Z",
     "iopub.status.idle": "2021-11-09T21:49:31.180059Z",
     "shell.execute_reply": "2021-11-09T21:49:31.179409Z",
     "shell.execute_reply.started": "2021-11-09T21:49:31.164327Z"
    },
    "id": "EthY01Ln006G"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T21:49:31.181665Z",
     "iopub.status.busy": "2021-11-09T21:49:31.181417Z"
    },
    "id": "vkZhtNHa_qVb",
    "outputId": "3f7fa5fd-8c54-4e9a-def4-9ed64a63cd57"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_tf,\n",
    "                    y_train2,\n",
    "                    validation_data=(X_valid_tf, y_valid2),\n",
    "                    epochs = 10,\n",
    "                    callbacks=[tensorboard_callback,\n",
    "                               early_stopping_monitor,\n",
    "                               model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "\n",
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCgVE-v0iSkA"
   },
   "outputs": [],
   "source": [
    "X_test_tf = tokenizer.texts_to_sequences(X_test2)\n",
    "X_test_tf = pad_sequences(X_test_tf, maxlen=max_seq)\n",
    "pred_tf = model.predict(X_test_tf)\n",
    "\n",
    "pred_tf = np.array([1 if pred >=0.5 else 0 for pred in pred_tf])\n",
    "\n",
    "unique, counts = np.unique(pred_tf, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)\n",
    "# pred_tf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_rep_tf = classification_report(y_test2, pred_tf)\n",
    "cm_tf = confusion_matrix(y_test2, pred_tf)\n",
    "\n",
    "cm_matrix_tf = pd.DataFrame(data=cm_tf, columns=['Predict Fake', 'Predict True'], \n",
    "                                 index=['Actual Fake', 'Actual True'])\n",
    "\n",
    "print(class_rep_tf)\n",
    "\n",
    "sns.heatmap(cm_matrix_tf, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAjI_AYa1Bcm",
    "outputId": "e7cd41f9-d9f5-40b7-b702-53169fc4fd53"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBhcF_C_1635"
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(word_index):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybkuDhhA2bfq",
    "outputId": "35c015b8-cbd1-4462-e76f-db2e915810f7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6kCIE_D2uIH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
